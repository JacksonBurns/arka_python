{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e8209d0",
   "metadata": {},
   "source": [
    "# `arka_carbonmangels`\n",
    "\n",
    "This notebook implements the [__Arithmetic Residuals in K-groups Analysis (ARKA)__](https://doi.org/10.1039/D4EM00173G) method in Python and then demonstrates the data on the Theraputic Data Commons [CYP2D6 Substrate CarbonMangels](https://tdcommons.ai/benchmark/admet_group/14cyp2d6s/) benchmark (via TDC and [`polaris`](https://polarishub.io/benchmarks/tdcommons/cyp2c9-substrate-carbonmangels)).\n",
    "\n",
    "This method takes a collection of molecular descriptors and projects them into a lower dimension for subsequent regression.\n",
    "The inline comments explain things in greater detail!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "104b2b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mordred import Calculator, descriptors\n",
    "from rdkit.Chem import MolFromSmiles\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150ba3d2",
   "metadata": {},
   "source": [
    "## Descriptor Calculator\n",
    "\n",
    "We'll start by defining a basic molecular descriptor calculator function.\n",
    "For this demo we will use [`mordredcommunity`](https://github.com/JacksonBurns/mordred-community), a community-maintained fork of the [Mordred descriptor calculator](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-018-0258-y).\n",
    "\n",
    "This function takes a list of SMILES strings and returns DataFrame of the 1,613 descriptors for each molecule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2819821b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mordred_features(smiles_list):\n",
    "    calc = Calculator(descs=descriptors, ignore_3D=True)\n",
    "    mols = [MolFromSmiles(smiles) for smiles in smiles_list]\n",
    "    features = calc.pandas(mols).fill_missing()\n",
    "    features.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a4837f",
   "metadata": {},
   "source": [
    "## Benchmark Data\n",
    "\n",
    "Based on which library you have installed, you can get to the training data using the below code - just set `DATASET_SOURCE` appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8fb665f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SOURCE = \"pytdc\"  # \"polaris\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9279e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n"
     ]
    }
   ],
   "source": [
    "if DATASET_SOURCE == \"polaris\":\n",
    "    import polaris as po\n",
    "    \n",
    "    benchmark = po.load_benchmark(\"tdcommons/cyp2d6-substrate-carbonmangels\")\n",
    "    smiles_col = list(benchmark.input_cols)[0]\n",
    "    target_col = list(benchmark.target_cols)[0]\n",
    "    train, test = benchmark.get_train_test_split()\n",
    "    train_df = train.as_dataframe()\n",
    "    test_df = test.as_dataframe()\n",
    "elif DATASET_SOURCE == \"pytdc\":\n",
    "    from tdc.benchmark_group import admet_group\n",
    "\n",
    "    group = admet_group(path = 'tdc_data/')\n",
    "    benchmark = group.get('cyp2d6-substrate-carbonmangels')\n",
    "    train_df, test_df = benchmark['train_val'], benchmark['test']\n",
    "    smiles_col = \"Drug\"\n",
    "    target_col = \"Y\"\n",
    "else:\n",
    "    raise RuntimeError(\"huh?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d693ed8",
   "metadata": {},
   "source": [
    "Now let's call that function on the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d14f70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 532/532 [01:15<00:00,  7.01it/s]\n"
     ]
    }
   ],
   "source": [
    "train_df['mordred_features'] = mordred_features(train_df[smiles_col].tolist()).values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a429be90",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = np.vstack(train_df['mordred_features'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5972d10d",
   "metadata": {},
   "source": [
    "## ARKA Procedure\n",
    "\n",
    "First, we rescale the training data using `MinMaxScaler` to range between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18dc6674",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(train_features)\n",
    "X = scaler.transform(train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07234745",
   "metadata": {},
   "source": [
    "We use the labels of the training data to separate these features into two groups (and pull out the targets, for later):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6edaba5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train_df[target_col].values\n",
    "y = train_df[target_col].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "924b90ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_0 = X[labels == 0]\n",
    "group_1 = X[labels == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714192cf",
   "metadata": {},
   "source": [
    "Now we calculate the per-feature mean within each group, then calculate the difference between the two groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "655da7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_0_means = group_0.mean(axis=0)\n",
    "group_1_means = group_1.mean(axis=0)\n",
    "mean_diff = group_0_means - group_1_means\n",
    "mean_diff_abs = np.abs(mean_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5257a335",
   "metadata": {},
   "source": [
    "From there, we separate the features into two \"classes\" based on their sign:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ac97f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_0_features = mean_diff > 0\n",
    "class_1_features = mean_diff < 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bc07f0",
   "metadata": {},
   "source": [
    "The paper is slightly ambiguous here - we haven't used the absolute mean differences, but it makes sense to use them in the denominator here when calculating the weights.\n",
    "That's what I'll do here, using indexing into the mean arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3db3878",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_0_weights = mean_diff[class_0_features] / mean_diff_abs[class_0_features].sum()\n",
    "class_1_weights = mean_diff[class_1_features] / mean_diff_abs[class_1_features].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d311f2cc",
   "metadata": {},
   "source": [
    "This block isn't in the original paper, but is alluded to having been done in previous work.\n",
    "In short, we need to downselect the features we have available.\n",
    "To do so, we'll just select those in the top 10% of weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c7e82a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_0_threshold = np.percentile(np.abs(class_0_weights), 90)\n",
    "top_1_threshold = np.percentile(np.abs(class_1_weights), 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341548a0",
   "metadata": {},
   "source": [
    "Now again, with some indexing, we touch up the weights to reflect this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "784807dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is easy to write and fast but not space efficient\n",
    "class_0_weights = np.where(np.abs(class_0_weights) >= top_0_threshold, class_0_weights, 0)\n",
    "class_1_weights = np.where(np.abs(class_1_weights) >= top_1_threshold, class_1_weights, 0)\n",
    "# normalize weights again\n",
    "class_0_weights /= np.abs(class_0_weights).sum()\n",
    "class_1_weights /= np.abs(class_1_weights).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f181b3",
   "metadata": {},
   "source": [
    "Finally, we calculate the actual ARKA features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8fcad13",
   "metadata": {},
   "outputs": [],
   "source": [
    "arka_0 = (X[:, class_0_features] * class_0_weights).sum(axis=1)\n",
    "arka_1 = (X[:, class_1_features] * class_1_weights).sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d83ca3b",
   "metadata": {},
   "source": [
    "## Fitting\n",
    "\n",
    "Now we simply fit whatever regression we want on these two features against our target vector `y`.\n",
    "In this demo I'm using Random Forest, because it's robust (read: foolproof).\n",
    "\n",
    "For TDC we need to provide 5 different sets of predictions to show the variability in the method.\n",
    "This isn't required for Polaris, but we can use the models as an ensemble instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "382dd3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = [RandomForestClassifier(random_state=42+i) for i in range(5)]\n",
    "for clf in clfs:\n",
    "    clf.fit(np.column_stack((arka_0, arka_1)), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4d3ebd",
   "metadata": {},
   "source": [
    "Just as a sanity check, we'll make sure we have good accuracy on the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf83be44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfs[0].score(np.column_stack((arka_0, arka_1)), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec16c348",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "To make predictions on the held out test data, we will go through the same procedure as above while re-using the weights we already calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f950579",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/135 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:42<00:00,  3.18it/s]\n"
     ]
    }
   ],
   "source": [
    "test_df['mordred_features'] = mordred_features(test_df[smiles_col].tolist()).values.tolist()\n",
    "test_features = np.vstack(test_df['mordred_features'].values)\n",
    "X_test = scaler.transform(test_features)\n",
    "X_test_arka_0 = (X_test[:, class_0_features] * class_0_weights).sum(axis=1)\n",
    "X_test_arka_1 = (X_test[:, class_1_features] * class_1_weights).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0863c39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [clf.predict(np.column_stack((X_test_arka_0, X_test_arka_1))) for clf in clfs]\n",
    "probabilities = [clf.predict_proba(np.column_stack((X_test_arka_0, X_test_arka_1)))[:, 1].flatten() for clf in clfs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a77f7a",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Based on the backend we're using to get to the data, we can now check the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cdec0e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET_SOURCE == \"polaris\":\n",
    "    probabilities = np.mean(probabilities, axis=0)\n",
    "    predictions = (probabilities > 0.5).astype(int)\n",
    "    results = benchmark.evaluate(predictions, probabilities).results\n",
    "elif DATASET_SOURCE == \"pytdc\":\n",
    "    results = group.evaluate_many([{benchmark['name']: p} for p in probabilities])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d31c28f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cyp2d6_substrate_carbonmangels': [0.548, 0.014]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arka_pytdc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
